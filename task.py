# -*- coding: utf-8 -*-
"""Task

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QSVg0ZZx-aXjxjYiKrktaDvw4vCBViaX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_regression(n_samples = 500, n_features = 2, n_informative = 1, noise = 30, random_state = 0)

print(X.shape)
print(y.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)

from sklearn.linear_model import LinearRegression
LR = LinearRegression()

LR.fit(X_train, y_train)

LR.coef_

LR.intercept_

epochs = 10000
lr = 0.001

n = 0
m = 0
b = 0

data1 = X_train[0][0]
data2 = (i[1] for i in X_train)

for i in range(epochs):

  y_pred = b + m * X_train[:, 0].reshape(len(X_train)) + n * X_train[:, -1].reshape(len(X_train))

  loss = sum(y_train - y_pred) ** 2 / len(X_train)

  d_m = (-2 / len(X_train)) * sum((y_train - y_pred) * X_train[:, 0])

  d_n = (-2 / len(X_train)) * sum((y_train - y_pred) * X_train[:, -1])

  d_b = (-2 / len(X_train)) * sum(y_train - y_pred)

  

  m = m - (lr * d_m)

  b = b - (lr * d_b)

  n = n - (lr * d_n)

  print("The loss after ", i , "iteration is ", loss)

data = (i[0] for i in X_train)

data1 = X_train[:, 0]
print(data1.shape)

X_train

# Create Class for Gradient Descent



class Gradient_descent:


  # def __init__(self, no_samples, no_features, testSize, randomState):

  #   self.create_linear_regression(no_samples, no_features, testSize, randomState)

  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  from sklearn import datasets


  # def create_linear_regression(no_samples, no_features, testSize, randomState):

  #   import numpy as n
  #   import pandas as pd
  #   import matplotlib.pyplot as plt
  #   from sklearn import datasets
  #   X, y = datasets.make_regression(n_samples = no_samples, n_features = no_features, n_informative = 1, noise = 30, random_state = 0)

  #   gradient_descent(no_features, testSize, randomState, X, y)
  
  # def gradient_descent(no_features, testSize, randomState, X, y):
  #   from sklearn.model_selection import train_test_split
  #   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testSize, random_state = randomState)

  #   from sklearn.linear_model import LinearRegression
  #   LR = LinearRegression()

  #   LR.fit(X_train, y_train)

    

  #   constant_list = []
  #   trained_features = []

  #   for i in range(no_features + 1):
  #     j = "w" + str(i)
  #     constant_list.append(j)

  #     feature = X_train[:, i].reshape(len(X_train))
  #     trained_features.append(feature)


  #   result_gradient(constant_list,trained_features, X_train, X_test, y_train, y_test)



  def result_gradient(constant_list,trained_features, X_train, X_test, y_train, y_test):

    epochs = 10000
    lr = 0.001
    c = 0

    parameters = []
    
    # y_pred = c + m * X_train[:, 0].reshape(len(X_train)) + n * X_train[:, -1].reshape(len(X_train))

    # loss = sum(y_train - y_pred) ** 2 / len(X_train)

    # d_c = (-2 / len(X_train)) * sum(y_train - y_pred)

    result_dict = {}

    for iteration in range(epochs):

      parameters = []

      if iteration == 0:
        for i in range(len(trained_features) + 1):
          constant = constant_list[i] = 0
          k = constant * trained_features[i]
          parameters.append(k)
      else:

        derivative_variable_values

        for i in range(len(trained_features) + 1):
          constant = derivative_variable_values[i]
          k = constant * trained_features[i]
          parameters.append(k)

      y_pred = c
      for i in parameters:
        y_pred += i

      loss = sum(y_train - y_pred) ** 2 / len(X_train)

      d_c = (-2 / len(X_train)) * sum(y_train - y_pred)   

      c = c - (lr * d_c)
      derivative_variable = []

      for i in range(len(trained_features) + 1):
        i = str(i)

        variable = d_ + i
        derivative_variable.append(variable)

      derivative_variable_values = {}
      for i in range(len(trained_features) + 1): 
        value = derivative_variable[i] = (-2 / len(X_train)) * sum((y_train - y_pred) * trained_features[i])
        value_constants = constant_list[i] = constant_list[i] - (lr * value)

        derivative_variable_values[value_constants]


      result_dict[i] = loss

      # print("The loss after ", i , "iteration is ", loss)


    return result_dict


  def gradient_descent(no_features, testSize, randomState, X, y):
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testSize, random_state = randomState)

    from sklearn.linear_model import LinearRegression
    LR = LinearRegression()

    LR.fit(X_train, y_train)

    

    constant_list = []
    trained_features = []

    for i in range(no_features + 1):
      j = "w" + str(i)
      constant_list.append(j)

      feature = X_train[:, i].reshape(len(X_train))
      trained_features.append(feature)


    result_gradient(constant_list,trained_features, X_train, X_test, y_train, y_test)


  def create_linear_regression(no_features = 2, testSize = 0.2, randomState = 1):

    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn import datasets
    X, y = datasets.make_regression(n_samples = 500, n_features = no_features, n_informative = 1, noise = 30, random_state = 0)

    gradient_descent(no_features, testSize, randomState, X, y)

object.create_linear_regression(no_features = 2, testSize = 0.2, randomState = 1)

class Gradient_descent:
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn import datasets

    

    X, y = datasets.make_regression(n_samples = 100, n_features = 3, n_informative = 1, noise = 30, random_state = 0)

    # gradient_descent(no_features, testSize, randomState, X, y)
    



    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)

    from sklearn.linear_model import LinearRegression
    LR = LinearRegression()

    LR.fit(X_train, y_train)

    # print("The values of X_train ---->", X_train)

    constant_list = []
    trained_features = []

    for i in range(3):
        j = "w" + str(i)
        constant_list.append(j)

        feature = X_train[:, i].reshape(len(X_train))
        # feature = X_train[:, i]
        # print(feature)
        trained_features.append(feature)




    epochs = 10000
    lr = 0.001
    c = 0

    parameters = []


    result_dict = {}

    for iteration in range(epochs):

        parameters = []

        if iteration == 0:
            for i in range(3):
                constant = constant_list[i] = 0
                k = constant * trained_features[i]
                parameters.append(k)
        else:

            derivative_variable_values

            for i in range(3):
                constant = derivative_variable_values[i]
                k = constant * trained_features[i]
                parameters.append(k)

        y_pred = c
        for i in parameters:
            y_pred += i

        loss = sum(y_train - y_pred) ** 2 / len(X_train)

        d_c = (-2 / len(X_train)) * sum(y_train - y_pred)   

        c = c - (lr * d_c)
        derivative_variable = []

        for i in range(3):
            i = str(i)

            variable = "d_" + i
            derivative_variable.append(variable)

        # print("derivative_variable---->",derivative_variable)
        derivative_variable_values = []
        for j in range(3):
            # print(derivative_variable[1]) 
            # print(trained_features[1])


            value = derivative_variable[j] = (-2 / len(X_train)) * sum((y_train - y_pred) * trained_features[j])
            # print(value)
            value_constants = constant_list[j] = constant_list[j] - (lr * value)

            derivative_variable_values.append(value_constants)


            result_dict[iteration] = loss

        result_dict

      # print("The loss after ", i , "iteration is ", loss)


    result_dict

obj = Gradient_descent()

print(obj.result_dict)